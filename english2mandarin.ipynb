{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b08c9da2-af1d-40ce-9c25-21bef5ae9b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from timeit import default_timer as timer\n",
    "from torch.nn import Transformer\n",
    "from torch import Tensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146b7140-68aa-405f-b206-822c7cfff519",
   "metadata": {},
   "source": [
    "## Create Token Dict for SRC and TGT Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2bcc95e-a7f2-49be-9e5e-e6966659326a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m948.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting zh-core-web-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/zh_core_web_sm-3.7.0/zh_core_web_sm-3.7.0-py3-none-any.whl (48.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 MB\u001b[0m \u001b[31m900.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from zh-core-web-sm==3.7.0) (3.7.4)\n",
      "Requirement already satisfied: spacy-pkuseg<0.1.0,>=0.0.27 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from zh-core-web-sm==3.7.0) (0.0.33)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->zh-core-web-sm==3.7.0) (1.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('zh_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# download the required package\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "!python3 -m spacy download zh_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "258808de-0dd5-45fb-aff2-e63a9e7efa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANGUAGE = 'en'\n",
    "TGT_LANGUAGE = 'zh'\n",
    "\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='zh_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c355d28c-c82b-40a0-ae2a-dfc068936e28",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "647db205-4575-4a0a-8cbd-1007a3f1ad16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Using cached pyarrow-16.0.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from datasets) (4.66.4)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.9.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
      "  Downloading huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from datasets) (24.0)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Downloading PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from requests>=2.19.0->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from requests>=2.19.0->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-2.19.0-py3-none-any.whl (542 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached aiohttp-3.9.5-cp312-cp312-macosx_11_0_arm64.whl (392 kB)\n",
      "Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pyarrow-16.0.0-cp312-cp312-macosx_11_0_arm64.whl (26.0 MB)\n",
      "Downloading PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl (165 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.6/165.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached xxhash-3.4.1-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached frozenlist-1.4.1-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\n",
      "Using cached multidict-6.0.5-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\n",
      "Using cached yarl-1.9.4-cp312-cp312-macosx_11_0_arm64.whl (79 kB)\n",
      "Installing collected packages: xxhash, pyyaml, pyarrow-hotfix, pyarrow, multidict, frozenlist, dill, attrs, yarl, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 attrs-23.2.0 datasets-2.19.0 dill-0.3.8 frozenlist-1.4.1 huggingface-hub-0.23.0 multidict-6.0.5 multiprocess-0.70.16 pyarrow-16.0.0 pyarrow-hotfix-0.6 pyyaml-6.0.1 xxhash-3.4.1 yarl-1.9.4\n",
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): Traceback (most recent call last):\n",
      "  File \"/Users/yifan/anaconda3/envs/eng2man/bin/huggingface-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 51, in main\n",
      "    service.run()\n",
      "  File \"/Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages/huggingface_hub/commands/user.py\", line 98, in run\n",
      "    login(token=self.args.token, add_to_git_credential=self.args.add_to_git_credential)\n",
      "  File \"/Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages/huggingface_hub/_login.py\", line 115, in login\n",
      "    interpreter_login(new_session=new_session, write_permission=write_permission)\n",
      "  File \"/Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages/huggingface_hub/_login.py\", line 191, in interpreter_login\n",
      "    token = getpass(\"Enter your token (input will not be visible): \")\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yifan/anaconda3/envs/eng2man/lib/python3.12/getpass.py\", line 77, in unix_getpass\n",
      "    passwd = _raw_input(prompt, stream, input=input)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yifan/anaconda3/envs/eng2man/lib/python3.12/getpass.py\", line 146, in _raw_input\n",
      "    line = input.readline()\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# download datasets from HuggingFace\n",
    "# !pip install datasets\n",
    "\n",
    "# add tokens from HuggingFace (https://huggingface.co/settings/tokens)\n",
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32dae85d-ca42-4173-bb99-20a1486f1cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             English  \\\n",
      "0  For greater sharpness, but with a slight incre...   \n",
      "1  He calls the Green Book, his book of teachings...   \n",
      "2  And the light breeze moves me to caress her lo...   \n",
      "3  They have the blood of martyrs is the White to...   \n",
      "4  Finally, the Lakers head to the Motor City to ...   \n",
      "\n",
      "                                  Mandarin  \n",
      "0   为了更好的锐度，但是附带的会多一些颗粒度，可以使用这个显影剂的1：1稀释液。  \n",
      "1               他还把宣扬自己思想的所谓《绿皮书》称作“新福音书”。  \n",
      "2                            微风推着我去爱抚它的长耳朵  \n",
      "3                           它们的先烈们的鲜血是白流了…  \n",
      "4  最后，在1月31日，湖人将前往汽车城底特律挑战活塞队，活塞近来在东部排名第二。  \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load datasets with the format \"user_name/dataset_name\"\n",
    "dataset = load_dataset(\"SouthernCrossAI/English_to_Mandarin\")\n",
    "\n",
    "# only use CSV_RATIO of the total data\n",
    "CSV_RATIO = 0.20\n",
    "csv = pd.DataFrame(dataset['train'])\n",
    "csv = csv[:int(len(csv) * CSV_RATIO)]\n",
    "print(csv.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "085a5a1c-c235-4741-b088-c674999fe6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for the later usage\n",
    "# csv.to_csv('translation2019zh_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dff47aa-0e49-4461-92c3-185549bf226d",
   "metadata": {},
   "source": [
    "## Split Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8921cb2-9951-4208-af2e-f5b9464ed22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size: 929057\n",
      "test set size: 103229\n"
     ]
    }
   ],
   "source": [
    "# use TEST_SET_RATIO of the total loaded data as the test set\n",
    "TEST_SET_RATIO = 0.1\n",
    "train_csv, test_csv = train_test_split(csv, test_size=TEST_SET_RATIO)\n",
    "print(\"training set size: {}\\ntest set size: {}\".format(train_csv.__len__(), test_csv.__len__()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9944a611-1493-45ba-bd92-dd432ebd1994",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Define Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "747950f7-644c-4b01-a93e-c416bd16650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, csv):\n",
    "        self.csv = csv\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return(self.csv['English'].iloc[idx], self.csv['Mandarin'].iloc[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee98db6-6065-4d00-a14a-718c9e86a2d3",
   "metadata": {},
   "source": [
    "## Initialise Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3a6f6ad-4999-411e-bf57-519b4328a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TranslationDataset(train_csv)\n",
    "valid_dataset = TranslationDataset(test_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26eab40-c600-4316-883a-e8392ab5d03c",
   "metadata": {},
   "source": [
    "## Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d4817d4-fbc0-4f5b-ab84-7250b609d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to yield list of tokens.\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices.\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab.\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Create torchtext's Vocab object.\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(\n",
    "        yield_tokens(train_dataset, ln),\n",
    "        min_freq=1,\n",
    "        specials=special_symbols,\n",
    "        special_first=True,\n",
    "    )\n",
    "\n",
    "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51f47d97-90ab-4be7-9dd6-10a6940647c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# `src` and `tgt` language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], # Tokenization\n",
    "                                               vocab_transform[ln], # Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "# function to collate data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5842534c-98eb-465f-9f1d-5c408832a356",
   "metadata": {},
   "source": [
    "## Define Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64e7830c-046c-496c-96a4-6fac1ddb39c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 192\n",
    "NHEAD = 6\n",
    "FFN_HID_DIM = 192\n",
    "BATCH_SIZE = 192\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "DEVICE = 'cpu'\n",
    "NUM_EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b160ad8-30bb-48f4-bb66-cadc94ddd02d",
   "metadata": {},
   "source": [
    "## Create Masks for SRC and TGT Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9fe171d-65a7-40a5-8469-9270ae54c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[1]\n",
    "    tgt_seq_len = tgt.shape[1]\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "    src_padding_mask = (src == PAD_IDX)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d0b2ac-d555-49d1-9303-02c7d27d853b",
   "metadata": {},
   "source": [
    "## Create a Positional Encoding Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37fdb192-f16a-4362-94c8-25ef017b78f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        \"\"\"\n",
    "        :param max_len: Input length sequence.\n",
    "        :param d_model: Embedding dimension.\n",
    "        :param dropout: Dropout value (default=0.1)\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Inputs of forward function\n",
    "        :param x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b90dbf8-4318-4d6f-918b-67395323cefd",
   "metadata": {},
   "source": [
    "## Create an Embedding Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e77d7ac3-b925-41b5-8d5d-2f225a0a4430",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277412bc-c4bb-4d9d-adab-7581f003a8c0",
   "metadata": {},
   "source": [
    "## Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d59fbb72-4515-4ba4-ac5d-1bef8de56ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_encoder_layers: int,\n",
    "        num_decoder_layers: int,\n",
    "        emb_size: int,\n",
    "        nhead: int,\n",
    "        src_vocab_size: int,\n",
    "        tgt_vocab_size: int,\n",
    "        dim_feedforward: int = 512,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(\n",
    "            d_model=emb_size,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453cb5a6-9700-4697-abae-48892be4bb7b",
   "metadata": {},
   "source": [
    "## Show the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b5193dc-f412-408b-aea1-13bd8e52ca3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321,184,253 total parameters.\n",
      "321,184,253 training parameters.\n",
      "Seq2SeqTransformer(\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=192, out_features=192, bias=True)\n",
      "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (generator): Linear(in_features=192, out_features=652925, bias=True)\n",
      "  (src_tok_emb): TokenEmbedding(\n",
      "    (embedding): Embedding(354286, 192)\n",
      "  )\n",
      "  (tgt_tok_emb): TokenEmbedding(\n",
      "    (embedding): Embedding(652925, 192)\n",
      "  )\n",
      "  (positional_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Seq2SeqTransformer(\n",
    "    NUM_ENCODER_LAYERS,\n",
    "    NUM_DECODER_LAYERS,\n",
    "    EMB_SIZE,\n",
    "    NHEAD,\n",
    "    SRC_VOCAB_SIZE,\n",
    "    TGT_VOCAB_SIZE,\n",
    "    FFN_HID_DIM\n",
    ").to(DEVICE)\n",
    "\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c5cce8-23ca-4e0f-af57-06d80e66afc9",
   "metadata": {},
   "source": [
    "## Define the Loss Function and Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f7864d5-cffb-4d38-9c67-435e96ee5466",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3d26ec-d10c-4eca-8b1e-fbb768a6fca5",
   "metadata": {},
   "source": [
    "## Define DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34b9f6d8-9f17-4f72-8f5e-04b231f51d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790f02c8-88ea-4fba-8c23-05584b0a9cfa",
   "metadata": {},
   "source": [
    "## Define the Training Loop and Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d8d9719-b6ff-428c-925e-c91dcd08b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer):\n",
    "    print('Training')\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for src, tgt in tqdm(train_dataloader, total=len(list(train_dataloader))):\n",
    "        # print(\" \".join(vocab_transform[SRC_LANGUAGE].lookup_tokens(list(src[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "        # print(\" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        logits = model(\n",
    "            src,\n",
    "            tgt_input,\n",
    "            src_mask,\n",
    "            tgt_mask,\n",
    "            src_padding_mask,\n",
    "            tgt_padding_mask,\n",
    "            src_padding_mask\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        loss = loss_fn(logits.view(-1, TGT_VOCAB_SIZE), tgt_out.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "    return losses / len(list(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e68126-9753-4bd9-bfcb-13c0087258d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    print('Validating')\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    for src, tgt in tqdm(val_dataloader, total=len(list(val_dataloader))):\n",
    "        # print(\" \".join(vocab_transform[SRC_LANGUAGE].lookup_tokens(list(src[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "        # print(\" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt[0].cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(\n",
    "            src,\n",
    "            tgt_input,\n",
    "            src_mask,\n",
    "            tgt_mask,\n",
    "            src_padding_mask,\n",
    "            tgt_padding_mask,\n",
    "            src_padding_mask\n",
    "        )\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        loss = loss_fn(logits.view(-1, TGT_VOCAB_SIZE), tgt_out.contiguous().view(-1))\n",
    "        losses += loss.item()\n",
    "    return losses / len(list(val_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5eb5bd-14bb-4fb0-8afa-62638de13af6",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7985e486-3988-4a7c-bc0a-987eeda3c552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4839 [00:00<?, ?it/s]/Users/yifan/anaconda3/envs/eng2man/lib/python3.12/site-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_loss_list, valid_loss_list = [], []\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(model, optimizer)\n",
    "    valid_loss = evaluate(model)\n",
    "    end_time = timer()\n",
    "    train_loss_list.append(train_loss)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {valid_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s \\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a37a3bd-6a6d-4cd5-9216-a1217046076c",
   "metadata": {},
   "source": [
    "## Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7a9231-2c65-4260-9771-2e01f8755582",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('outputs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a715d74-cfad-4239-929a-1617c7f68e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'outputs/model_ratio020_epoch40.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b16b48-834b-4c9a-ab55-4b93b07c31c9",
   "metadata": {},
   "source": [
    "## Save the Loss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d2f85-4783-4aae-810a-bf9a816fe422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plots(train_loss, valid_loss):\n",
    "    \"\"\"\n",
    "    Function to save the loss plots to disk.\n",
    "    \"\"\"\n",
    "    # Loss plots.\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(\n",
    "        train_loss, color='blue', linestyle='-',\n",
    "        label='train loss'\n",
    "    )\n",
    "    plt.plot(\n",
    "        valid_loss, color='red', linestyle='-',\n",
    "        label='validataion loss'\n",
    "    )\n",
    "    plt.xticks(range(len(train_loss)))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join('outputs', 'loss.png'))\n",
    "    plt.show()\n",
    "\n",
    "save_plots(train_loss_list, valid_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8e00b4-3672-4e72-bb40-07b29b09111f",
   "metadata": {},
   "source": [
    "## Test the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c4e337-0618-47f9-ae6d-f98f36137491",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('outputs/model_ratio020_epoch40.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9060fb-8e9b-4468-b52e-e9cbe925bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to generate output sequence using greedy algorithm.\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        if i == 0:\n",
    "            ys = ys.transpose(1, 0)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(1))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "# Translation function.\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(1, -1)\n",
    "    num_tokens = src.shape[1]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46be529-5fdf-4ec0-908d-7d5fc7eaa772",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SRC, GT pairs from the validation set\n",
    "infer_sentences = [\n",
    "    [\"Slowly and not without struggle, America began to listen.\", \"美国缓慢地开始倾听，但并非没有艰难曲折。\"],\n",
    "    [\"Dithering is a technique that blends your colors together, making them look smoother, or just creating interesting textures.\", \"抖动是关于颜色混合的技术，使你的作品看起来更圆滑，或者只是创作有趣的材质。\"],\n",
    "    [\"The second encounter relates to my grandfather's treasure box.\", \"第二次事件跟我爷爷的宝贝匣子有关。\"]\n",
    "]\n",
    "\n",
    "for sentence in infer_sentences:\n",
    "    print(f\"SRC: {sentence[0]}\")\n",
    "    print(f\"GT: {sentence[1]}\")\n",
    "    print(f\"PRED: {translate(model, sentence[0])}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eng2man",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
